{
  "metadata": {
    "name": "WhoWillDonate2",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf \u003d spark.read.parquet(\"hdfs://thankgod/tmp/charity-base-with-status.parquet\")\ndf \u003d df.filter(df[\"CustomerAge\"]\u003e0).drop(\"id\").drop(\"CustomerID\")\nsimple \u003d df.sample(.0236)\ndf.show(4)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.describe().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.types import StringType,IntegerType\nfrom pyspark.sql.functions import udf\ny_udf \u003d udf(lambda y: \"No\" if y\u003d\u003d\"0\" else \"Yes\",StringType())\nsimple \u003d df.withColumn(\"Charity\",y_udf(\"CharityStatus\")).drop(\"CharityStatus\")\nsimple.show(4)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndef age_multiple(age):\n    if(age \u003c\u003d 25):\n        return \u0027Under 25\u0027\n    elif (age \u003e\u003d 25 and age \u003c\u003d 35):\n        return \u0027Between 25 and 35\u0027\n    elif (age \u003e 35 and age \u003c 50):\n        return \u0027Between 36 and 49\u0027\n    elif(age \u003e\u003d 50):\n        return \u0027Over 50\u0027\n    else:\n        return \u0027N/A\u0027\n        \n        \nsimple \u003d simple.withColumn(\"CustomerAge\", simple[\"CustomerAge\"].cast(IntegerType()))\nage_multiple_udf \u003d udf(lambda age: age_multiple(age), StringType())\nspark.udf.register(\"age_multiple_udf\", age_multiple_udf)\nsimple \u003d simple.withColumn(\"CustomerAge\",age_multiple_udf(\"CustomerAge\"))\nsimple.show(4)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsimple.select(\"Charity\").show(4)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StandardScaler\n\nstringIndexer_State \u003d StringIndexer().setInputCol(\"State\").setOutputCol(\"State_index\")\nstringIndexer_CustomerZIPCode \u003d StringIndexer().setInputCol(\"CustomerZIPCode\").setOutputCol(\"CustomerZIPCode_index\")\nstringIndexer_CustomerAge \u003d StringIndexer().setInputCol(\"CustomerAge\").setOutputCol(\"CustomerAge_index\")\n\nstringIndexer_State.setHandleInvalid(\"keep\")\nstringIndexer_CustomerZIPCode.setHandleInvalid(\"keep\")\nstringIndexer_CustomerAge.setHandleInvalid(\"keep\")\n\nencoder_State \u003d OneHotEncoder(inputCol\u003d\"State_index\", outputCol\u003d\"State_encoded\")\nencoder_CustomerZIPCode \u003d OneHotEncoder(inputCol\u003d\"CustomerZIPCode_index\", outputCol\u003d\"CustomerZIPCode_encoded\")\nencoder_CustomerAge \u003d OneHotEncoder(inputCol\u003d\"CustomerAge_index\", outputCol\u003d\"CustomerAge_encoded\")\n\nlabel_indexer \u003d StringIndexer().setInputCol(\"Charity\").setOutputCol(\"label\")\nlabel_indexer.setHandleInvalid(\"keep\")\n\nassembler \u003d VectorAssembler().setInputCols([\"State_encoded\",\"CustomerZIPCode_encoded\",\"CustomerAge_encoded\"]).setOutputCol(\"vectorized_features\")\nscaler \u003d StandardScaler().setInputCol(\"vectorized_features\").setOutputCol(\"features\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml import Pipeline\n\npipeline_stages \u003d Pipeline().setStages([stringIndexer_State,stringIndexer_CustomerZIPCode,stringIndexer_CustomerAge,encoder_State,encoder_CustomerZIPCode,encoder_CustomerAge,assembler,label_indexer,scaler])\n\npipeline_model \u003d pipeline_stages.fit(simple)\npipeline_df \u003d pipeline_model.transform(simple)\npipeline_df.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npipeline_df.drop(\"vectorized_features\").show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npipeline_df.write.parquet(\"hdfs://thankgod/pipleline.charitystatus.parquet\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntrain, test \u003d pipeline_df.randomSplit([0.8,0.2],seed\u003d2018)\nprint(\"Training Dataset Count: \" + str(train.count()))\nprint(\"Test Dataset Count: \" + str(test.count()))"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntrain.groupby(\"Charity\").count().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.classification import LogisticRegression\nlr \u003d LogisticRegression(featuresCol\u003d\u0027features\u0027,labelCol\u003d\u0027label\u0027,maxIter\u003d5)\nlrModel \u003d lr.fit(train)\npredictions \u003d lrModel.transform(test)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npredictions.write.parquet(\"hdfs://thankgod/predictions.charitystatus.parquet\")"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npredictions.select(\u0027label\u0027,\u0027features\u0027,\u0027rawPrediction\u0027,\u0027prediction\u0027,\u0027probability\u0027).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\naccuracy \u003d predictions.filter(predictions.label \u003d\u003d predictions.prediction).count() / float(predictions.count())\nprint(\"Accuracy : \",accuracy)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(lr.explainParams())"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport matplotlib.pyplot as plt\nimport numpy as np\nbeta \u003d np.sort(lrModel.coefficients)\nplt.plot(beta)\nplt.ylabel(\u0027Beta Coefficients\u0027)\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(lrModel.coefficientMatrix)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}